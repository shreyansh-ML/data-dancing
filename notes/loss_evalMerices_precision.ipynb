{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://ml-cheatsheet.readthedocs.io/en/latest/loss_functions.html\n",
    "1.Cross Entropy log loss -- classification\n",
    "2.hinge -- classification\n",
    "3.huber -- regression\n",
    "4.Kullback-Leibler -- \n",
    "5.MAE [l1]\n",
    "5.MSE [l2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cross entropy - log loss\n",
    "for binar classification:\n",
    "  −(ylog(p)+(1−y)log(1−p))\n",
    "    it is derivation of sigmoid function .\n",
    "    \n",
    " for multiclass :\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://www.dataschool.io/simple-guide-to-confusion-matrix-terminology/\n",
    "sensitivity,specificity,precision,recall               \n",
    "\n",
    "sensitivity or True Positive Rate or recall = true positive / tp+fn\n",
    "\n",
    "Specificity = True Negatives / (True Negatives + False Positives) \n",
    "\n",
    "precision=tp/tp+fp\n",
    "\n",
    "recall=tp/tp+fn\n",
    "\n",
    "fp rate=fp/fp+tn\n",
    "False Positive Rate = 1 - Specificity\n",
    "False Positive Rate = False Positives / (False Positives + True Negatives) #false alarm rate ,inverted specificity\n",
    "ROC curves are appropriate when the observations are balanced between each class, whereas precision-recall curves are appropriate for imbalanced datasets.\n",
    "ROC is a plot of the false positive rate (x-axis) versus the true positive rate (y-axis) for a number of different candidate threshold values between 0.0 and 1.0\n",
    "\n",
    "When we decrease the threshold, we get more positive values thus increasing the sensitivity. Meanwhile, this will decrease the specificity.\n",
    "https://developers.google.com/machine-learning/crash-course/classification/roc-and-auc\n",
    "AUC is scale-invariant.\n",
    "AUC is classification-threshold-invariant.\n",
    "\n",
    "---\n",
    "However, both these reasons come with caveats, which may limit the usefulness of AUC in certain use cases:\n",
    "\n",
    "Scale invariance is not always desirable. For example, sometimes we really do need well calibrated probability outputs, and AUC won’t tell us about that.\n",
    "\n",
    "Classification-threshold invariance is not always desirable. In cases where there are wide disparities in the cost of false negatives vs. false positives, it may be critical to minimize one type of classification error. For example, when doing email spam detection, you likely want to prioritize minimizing false positives (even if that results in a significant increase of false negatives). AUC isn't a useful metric for this type of optimization.\n",
    "\n",
    "AUC is based on the relative predictions, so any transformation of the predictions that preserves the relative ranking has no effect on AUC. This is clearly not the case for other metrics such as squared error, log loss, or prediction bias"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "fpr, tpr, thresholds = metrics.roc_curve(y_test_np,y_pred, pos_label=2)\n",
    "pre,rec,thold=metrices.precision_recall_curve(actual_label,probality/decision fn score)\n",
    "\n",
    "look at following:\n",
    "from sklearn.metrics import precision_recall_curve\n",
    "from sklearn.metrics import plot_precision_recall_curve\n",
    "https://scikit-learn.org/stable/auto_examples/model_selection/plot_precision_recall.html#sphx-glr-auto-examples-model-selection-plot-precision-recall-py\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://machinelearningmastery.com/roc-curves-and-precision-recall-curves-for-classification-in-python/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://blog.insightdatascience.com/visualizing-machine-learning-thresholds-to-make-better-business-decisions-4ab07f823415"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The higher the norm index, the more it focuses on large values and neglects small\n",
    "ones. This is why the RMSE is more sensitive to outliers than the MAE. But when outliers are exponentially rare (like in a bell-shaped curve), the RMSE performs\n",
    "very well and is generally preferred."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "r2=1-SS(res)/SS(tot) \n",
    "r-square never decrease if we add new variables even if there is slight coorelation\n",
    "adjusted r2=1-(1-r2)n-1/n-p-1 : p-number of regressors (independent variables) n-sample size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "https://stackoverflow.com/questions/44172162/f1-score-vs-roc-auc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Regression Loss Functions: Squared Error Loss,Absolute Error Loss,Huber Loss\n",
    "    Binary Classification Loss Functions :Binary Cross-Entropy,Hinge Loss \n",
    "        Multi-class Classification Loss Functions:Multi-class Cross Entropy Loss,Kullback Leibler Divergence Loss"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
