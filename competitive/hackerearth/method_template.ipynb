{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "following are the target:\n",
    "   1. Implement pipeline\n",
    "   2. implement staking\n",
    "   3. use knn for imputation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import sklearn\n",
    "import numpy as np\n",
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#def model_eval_reg(model)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "columns=[]\n",
    "model_fs=XGBClassifier()\n",
    "kfold=StratifiedKFold(n_splits=10,random_state=1,shuffle=True)\n",
    "for i in X_train.columns:\n",
    "    columns.append(i)\n",
    "    X_Sel=X_train.loc[:,columns]\n",
    "    cv_results=cross_val_score(model_fs,X_Sel,y_train,cv=kfold,scoring='accuracy')\n",
    "    #print('mean: { 0}    std: {1}'.format(cv_results.mean(),cv_results.std()))\n",
    "    print(' %f (%f)' % ( cv_results.mean(), cv_results.std()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "models = []\n",
    "models.append(('LR', LogisticRegression(solver='liblinear', multi_class='ovr')))\n",
    "models.append(('LDA', LinearDiscriminantAnalysis()))\n",
    "models.append(('KNN', KNeighborsClassifier()))\n",
    "models.append(('CART', DecisionTreeClassifier()))\n",
    "models.append(('NB', GaussianNB()))\n",
    "models.append(('SVM', SVC(gamma='auto')))\n",
    "#RandomForest params\n",
    "max_feature=round(math.sqrt(len(X_train.columns)))\n",
    "min_sample_leaf_value=50\n",
    "models.append(('RandomForest',RandomForest(max_features=max_feature,min_sample_leaf=min_sample_leaf_value,oob_score=True)))\n",
    "#XGB params\n",
    "\n",
    "models.append(('xgboost',XGBClassifier()))\n",
    "# evaluate each model in turn\n",
    "results = []\n",
    "names = []\n",
    "kfold = StratifiedKFold(n_splits=10, random_state=1, shuffle=True)\n",
    "for name, model in models:\n",
    "    kfold = StratifiedKFold(n_splits=10, random_state=1, shuffle=True)\n",
    "    cv_results = cross_val_score(model, X_train, y_train, cv=kfold, scoring='accuracy')\n",
    "    results.append(cv_results)\n",
    "    names.append(name)\n",
    "    #print('%s: %f (%f)' % (name, cv_results.mean(), cv_results.std()))\n",
    "    #print(f'name:{name}   result_mean={cv_results.mean()}')\n",
    "    print('{0}     {1:.2f}'.format(name,cv_results.mean()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classification_model_evaluation(models,X_train,y_train,pcv=10,score='accuracy'):\n",
    "    result={}\n",
    "    for name,model in models:\n",
    "        cv_result=cross_val_score(model,X_train,y_train,cv=pcv,scoring=score)\n",
    "        result[name]=cv_result\n",
    "        print('{0}     {1:.2f}'.format(name,cv_result.mean()))\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "rfc=RandomForestClassifier(max_features=sqrt(n_features),mean_sample_leaf=50,oob_score=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
       "                       max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
       "                       min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                       min_samples_leaf=1, min_samples_split=2,\n",
       "                       min_weight_fraction_leaf=0.0, n_estimators=0,\n",
       "                       n_jobs=None, oob_score=False, random_state=None,\n",
       "                       verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Empirical good default values are max_features=n_features for regression problems, and max_features=sqrt(n_features) for classification tasks.usually max_features=n_features/3 is good default for regression problems\n",
    "https://blog.datadive.net/selecting-good-features-part-iii-random-forests/\n",
    "https://people.eecs.berkeley.edu/~jrs/189s17/lec/16.pdf\n",
    "if possible control max_depth by min_sample_leaf parameter\n",
    "min_samples_split need not be checked .Its default of 2 is good.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf=RandomForestClassifier()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LR_Param={}\n",
    "depth_val=np.linspace(4,64,5,endpoint=True)\n",
    "#num_of_estmators=[10,20,30]\n",
    "min_sample_leaf_val=[20,40,60,80,100]\n",
    "max_feature_val=[sqrt(n_features),n_features/3,n_features/2,n_features]\n",
    "#DecisionTree_param={'max_depth':depth,max_feature=max_feature_sel,min_sample_leaf=min_sample_leaf_val}\n",
    "#np.arange is also available\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def GridSearchRF(X_train,y_train):\n",
    "    RandomForest_Param={'max_depth':depth_val,'min_sample_leaf':min_sample_leaf_val,'max_feature'=max_feature_val}\n",
    "    rfc=RandomForestClassifier()\n",
    "    grid_search=GridSearchCV(estimator=rfc,param_grid=RandomForest_Param,cv=10)\n",
    "    grid_search.fit(X_train,y_train)\n",
    "    print(grid_search.best_param_)\n",
    "    rf=RandomForestClassifier(max_depth=best_param)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
