{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "following are the target:\n",
    "   1. Implement pipeline\n",
    "   2. implement staking\n",
    "   3. use knn for imputation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import sklearn\n",
    "import numpy as np\n",
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#def model_eval_reg(model)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "columns=[]\n",
    "model_fs=XGBClassifier()\n",
    "kfold=StratifiedKFold(n_splits=10,random_state=1,shuffle=True)\n",
    "for i in X_train.columns:\n",
    "    columns.append(i)\n",
    "    X_Sel=X_train.loc[:,columns]\n",
    "    cv_results=cross_val_score(model_fs,X_Sel,y_train,cv=kfold,scoring='accuracy')\n",
    "    #print('mean: { 0}    std: {1}'.format(cv_results.mean(),cv_results.std()))\n",
    "    print(' %f (%f)' % ( cv_results.mean(), cv_results.std()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "models = []\n",
    "models.append(('LR', LogisticRegression(solver='liblinear', multi_class='ovr')))\n",
    "models.append(('LDA', LinearDiscriminantAnalysis()))\n",
    "models.append(('KNN', KNeighborsClassifier()))\n",
    "models.append(('CART', DecisionTreeClassifier()))\n",
    "models.append(('NB', GaussianNB()))\n",
    "models.append(('SVM', SVC(gamma='auto')))\n",
    "#RandomForest params\n",
    "max_feature=round(math.sqrt(len(X_train.columns)))\n",
    "min_sample_leaf_value=50\n",
    "models.append(('RandomForest',RandomForest(max_features=max_feature,min_sample_leaf=min_sample_leaf_value,oob_score=True)))\n",
    "#XGB params\n",
    "\n",
    "models.append(('xgboost',XGBClassifier()))\n",
    "# evaluate each model in turn\n",
    "results = []\n",
    "names = []\n",
    "kfold = StratifiedKFold(n_splits=10, random_state=1, shuffle=True)\n",
    "for name, model in models:\n",
    "    kfold = StratifiedKFold(n_splits=10, random_state=1, shuffle=True)\n",
    "    cv_results = cross_val_score(model, X_train, y_train, cv=kfold, scoring='accuracy')\n",
    "    results.append(cv_results)\n",
    "    names.append(name)\n",
    "    #print('%s: %f (%f)' % (name, cv_results.mean(), cv_results.std()))\n",
    "    #print(f'name:{name}   result_mean={cv_results.mean()}')\n",
    "    print('{0}     {1:.2f}'.format(name,cv_results.mean()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classification_model_evaluation(models,X_train,y_train,pcv=10,score='accuracy'):\n",
    "    result={}\n",
    "    for name,model in models:\n",
    "        cv_result=cross_val_score(model,X_train,y_train,cv=pcv,scoring=score)\n",
    "        result[name]=cv_result\n",
    "        print('{0}     {1:.2f}'.format(name,cv_result.mean()))\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "rfc=RandomForestClassifier(max_features=sqrt(n_features),mean_sample_leaf=50,oob_score=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
       "                       max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
       "                       min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                       min_samples_leaf=1, min_samples_split=2,\n",
       "                       min_weight_fraction_leaf=0.0, n_estimators=0,\n",
       "                       n_jobs=None, oob_score=False, random_state=None,\n",
       "                       verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Empirical good default values are max_features=n_features for regression problems, and max_features=sqrt(n_features) for classification tasks.usually max_features=n_features/3 is good default for regression problems\n",
    "https://blog.datadive.net/selecting-good-features-part-iii-random-forests/\n",
    "https://people.eecs.berkeley.edu/~jrs/189s17/lec/16.pdf\n",
    "if possible control max_depth by min_sample_leaf parameter\n",
    "min_samples_split need not be checked .Its default of 2 is good.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#depth_val=np.linspace(4,64,5,endpoint=True)\n",
    "depth_val=[4,8,16,32]\n",
    "#num_of_estmators=[10,20,30]\n",
    "min_samples_leaf_val=[20,40,60,80,100]\n",
    "max_features_val=[3,2,1]\n",
    "#DecisionTree_param={'max_depth':depth,max_feature=max_feature_sel,min_sample_leaf=min_sample_leaf_val}\n",
    "#np.arange is also available"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def GridSearchRF(X_train,y_train):\n",
    "    RandomForest_Param={'max_depth':depth_val,'min_samples_leaf':min_samples_leaf_val,'max_features':max_features_val}\n",
    "    rfc=RandomForestClassifier()\n",
    "    grid_search=GridSearchCV(estimator=rfc,param_grid=RandomForest_Param,cv=10)\n",
    "    grid_search.fit(X_train,y_train)\n",
    "    print(grid_search.best_params_)\n",
    "    model=RandomForestClassifier(max_depth=grid_search.best_params_['max_depth'],max_features=grid_search.best_params_['max_features'],min_samples_leaf=grid_search.best_params_['min_samples_leaf'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://machinelearningmastery.com/configure-gradient-boosting-algorithm/\n",
    "https://machinelearningmastery.com/gentle-introduction-gradient-boosting-algorithm-machine-learning/\n",
    "https://www.linkedin.com/pulse/approaching-almost-any-machine-learning-problem-abhishek-thakur/\n",
    "## gradient boosting machine tuning\n",
    "tree specific parameters:\n",
    "-------------choose a high learning rate and find optimum number of trees for that tree\n",
    "-------------Tune tree specific parameter\n",
    "-------------change the learning rate and optimum number of  trees\n",
    "\n",
    "for base model: min_samples_split = 500 ,min_samples_leaf = 50,max_depth = 8 ,max_features = ‘sqrt’ ,subsample = 0.8\n",
    "tree Based parameters fro GBM:\n",
    "1.Tune max_depth and min_samples_split\n",
    "2.Tune min_samples_leaf\n",
    "3.Tune max_features\n",
    "   \n",
    "   -----------------\n",
    "   use early stopping :\n",
    "   validation_fraction=.2\n",
    "   n_iter_no_change=\n",
    "   tol\n",
    "   \n",
    "   return values:\n",
    "   feature_importances_\n",
    "   \n",
    "   \n",
    "   \n",
    "   ------------------------------------------------------\n",
    "   learning_rate=0.1 (shrinkage).\n",
    "n_estimators=100 (number of trees).\n",
    "max_depth=3.\n",
    "min_samples_split=2.\n",
    "min_samples_leaf=1.\n",
    "subsample=1.0."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## XGBoost tuning\n",
    " tree specific parameters: max_depth, min_child_weight, gamma, subsample, colsample_bytree,scale_pos_weight(because of class imbalance)\n",
    "four steps of xgb parameter tuning:\n",
    "    1.choose a high learning rate and  find the optimum number of trees for this learning rate\n",
    "    2.tune tree specific parameter : max_depth,min_child_weight,gamma,subsample,colsample_bytree\n",
    "    3. tune regularization parameters :lambda,alpha\n",
    "    4.lower the learning rate;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tree based parameters: max_depth = 5,min_child_weight = 1,gamma = 0,subsample, colsample_bytree = 0.8,scale_pos_weight = 1\n",
    "uning Regularization Parameters: reg_alpha,reg_lambda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from xgboost import XGBClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgbc=XGBClassifier(objective='multi:softmax',booster='gbtree',n_estimators=100,silent=0,learning_rate=.3,min_child_weight=1,max_depth=6,gamma=0,max_delta_setp=0,colsample_bytree=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
       "              colsample_bynode=1, colsample_bytree=1, gamma=0,\n",
       "              learning_rate=0.3, max_delta_setp=0, max_delta_step=0,\n",
       "              max_depth=6, min_child_weight=1, missing=None, n_estimators=100,\n",
       "              n_jobs=1, nthread=None, objective='multi:softmax', random_state=0,\n",
       "              reg_alpha=0, reg_lambda=1, scale_pos_weight=1, seed=None,\n",
       "              silent=0, subsample=1, verbosity=1)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xgbc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "param=xgbc.get_xgb_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method XGBModel.get_xgb_params of XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
       "              colsample_bynode=1, colsample_bytree=1, gamma=0,\n",
       "              learning_rate=0.3, max_delta_setp=0, max_delta_step=0,\n",
       "              max_depth=6, min_child_weight=1, missing=None, n_estimators=100,\n",
       "              n_jobs=1, nthread=None, objective='multi:softmax', random_state=0,\n",
       "              reg_alpha=0, reg_lambda=1, scale_pos_weight=1, seed=None,\n",
       "              silent=0, subsample=1, verbosity=1)>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xgbc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate_val=[0.01,0.02,0.04,0.08,.1,.2,.4,.8] #default .3\n",
    "subsample_val=[.5,.6,.8,1]\n",
    "colsample_bytree_val=[.5,.7,.9,1]\n",
    "n_estimators_val=[50,100,200]\n",
    "#min_child_weight_val=[1,2,3,4]\n",
    "#objective_val=['binary:logistic','multi:softmax','multi:softprob']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def GridSearchXGBC(X_train,y_train,lr=.3,trees=50):\n",
    "    XGB_classifier_param={'learning_rate':learning_rate_val,'subsample':subsample_val,'colsample_bytree':colsample_bytree_val}\n",
    "    xgbc=XGBClassifier(objective='multi:softmax',booster='gbtree',n_estimators=100,silent=0,learning_rate=.3,min_child_weight=1,max_depth=6,gamma=0,max_delta_setp=0,colsample_bytree=1)\n",
    "    gs=GridSearchCV(estimator=xgbc,param_grid=XGB_classifier_param,cv=10)\n",
    "    gs.fit(X_train,y_train)\n",
    "    print(gs.best_params_)\n",
    "    model=XGBClassifier(objective='multi:softmax',booster='gbtree',n_estimators=100,silent=0,learning_rate=gs.best_params_['learning_rate'],subsample=gs.best_params_['subsample_val'] ,min_child_weight=1,max_depth=6,gamma=0,max_delta_setp=0,colsample_bytree=gs.best_params_['cosample_bytree'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def GridSearchGBM(X_train,y_train,lr=.3,trees=50):\n",
    "    default_learning_rate=.3\n",
    "    default_num_of_tree=50\n",
    "    base_GBM=GradientBoostingClassifier(learning_rate=lr,n_estimators=trees,max_depth=6,max_features='sqrt',min_samples_split=50,min_samples_leaf=10,subsample=1.0,n_iter_no_change=20,validation_fraction=.2)\n",
    "    max_depth_val=[10,30,50,70,90,20]\n",
    "    #max_depth_val=[10]\n",
    "    min_samples_split_val=[4,8,16,32,64,128,256,512]\n",
    "    #min_samples_split_val=[4]\n",
    "    n_fe=len(X_Sel.columns)\n",
    "    max_features_val=[n_fe//2,n_fe//3,round(math.sqrt(n_fe)),n_fe]\n",
    "    tol=1e-4\n",
    "    first_tuning={'max_depth':max_depth_val,'min_samples_split':min_samples_split_val,'max_features':max_features_val}\n",
    "    grid_model=GridSearchCV(estimator=base_GBM,param_grid=first_tuning,cv=10)\n",
    "    grid_model.fit(X_train,y_train)\n",
    "    #learning-rate=2-10/num of trees\n",
    "    #learning_rate_val=[.2,.1,.05,.02,.01]\n",
    "    learning_rate_val=[.2,.1]\n",
    "    #n_estimators_val=[50,100,200,400,500,800,1000]\n",
    "    n_estimators_val=[100,200]\n",
    "    second_tuning={'learning_rate':learning_rate_val,'n_estimators':n_estimators_val}\n",
    "    final_model=GridSearchCV(estimator=grid_model.best_estimator_,param_grid=second_tuning,cv=10)\n",
    "    return final_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "gb=GradientBoostingClassifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GradientBoostingClassifier(criterion='friedman_mse', init=None,\n",
       "                           learning_rate=0.1, loss='deviance', max_depth=3,\n",
       "                           max_features=None, max_leaf_nodes=None,\n",
       "                           min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                           min_samples_leaf=1, min_samples_split=2,\n",
       "                           min_weight_fraction_leaf=0.0, n_estimators=100,\n",
       "                           n_iter_no_change=None, presort='auto',\n",
       "                           random_state=None, subsample=1.0, tol=0.0001,\n",
       "                           validation_fraction=0.1, verbose=0,\n",
       "                           warm_start=False)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gb\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_GBM=GradientBoostingClassifier(learning_rate=.3,n_estimators=50,max_depth=6,max_features='sqrt',min_samples_split=50,min_samples_leaf=10,subsample=1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def GridSearchXGBC(X_train,y_train,lr=.3,trees=50):\n",
    "    #XGB_classifier_param={'learning_rate':learning_rate_val,'subsample':subsample_val,'colsample_bytree':colsample_bytree_val}\n",
    "    base_XGBC=XGBClassifier(objective='multi:softmax',booster='gbtree',n_estimators=trees,silent=0,learning_rate=lr,min_child_weight=1,max_depth=6,gamma=0,max_delta_setp=0,colsample_bytree=1)\n",
    "    max_depth_val=[100]\n",
    "    #max_depth_val=[10,30,50,70,90,20]\n",
    "    min_child-weight_val=[1]\n",
    "    #min_child_weight_val=[1,2,4,8] #equivalent of min_child_leaf in GBM\n",
    "    #eta=learning_rate\n",
    "    first_tuning={'max_depth':max_depth_val,'min_child_weight':min_child_weight_val}\n",
    "    grid_model=GridSearchCV(estimator=base_XGBC,param_grid=first_tuning,cv=10)\n",
    "    grid_model.fit(X_train,y_train)\n",
    "    #learning-rate=2-10/num of trees\n",
    "    #learning_rate_val=[.2,.1,.05,.02,.01]\n",
    "    learning_rate_val=[.2,.1]\n",
    "    #n_estimators_val=[50,100,200,400,500,800,1000]\n",
    "    n_estimators_val=[100,200]\n",
    "    second_tuning={'learning_rate':learning_rate_val,'n_estimators':n_estimators_val}\n",
    "    final_model=GridSearchCV(estimator=grid_model.best_estimator_,param_grid=second_tuning,cv=10)\n",
    "    return final_model\n",
    "     \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    gs=GridSearchCV(estimator=xgbc,param_grid=XGB_classifier_param,cv=10)\n",
    "    gs.fit(X_train,y_train)\n",
    "    print(gs.best_params_)\n",
    "    model=XGBClassifier(objective='multi:softmax',booster='gbtree',n_estimators=100,silent=0,learning_rate=gs.best_params_['learning_rate'],subsample=gs.best_params_['subsample_val'] ,min_child_weight=1,max_depth=6,gamma=0,max_delta_setp=0,colsample_bytree=gs.best_params_['cosample_bytree'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ensemble(models,layers,X_train,y_train):\n",
    "    for index in np.arange(0,layers,1):\n",
    "        model=models[index]\n",
    "        X=X_train[index]\n",
    "        y=y_train[index]\n",
    "        X_test=X_train[index+1]\n",
    "        for m in models:\n",
    "            m.fit(X,y)\n",
    "            input=m.predict(X_test)\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ensemble_level_train(base_models,X_train,X_holdout,y_train,y_holdout):\n",
    "    intermediate_input=pd.DataFrame()\n",
    "    for model in base_models:\n",
    "        model.fit(X_train,y_train)\n",
    "        y_temp=model.predict(X_holdout)\n",
    "        intermediate_input[model.__class__.__name__]=y_temp;\n",
    "        print(model.__class__.__name__,accuracy_score(y_holdout,y_temp))\n",
    "    return intermediate_input\n",
    "    \n",
    "    \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ensemble_level_predict(base_models,X_test):\n",
    "    intermediate_input=pd.DataFrame()\n",
    "    for model in base_models:\n",
    "        y_temp=model.predict(X_test)\n",
    "        intermediate_input[model.__class__.__name__]=y_temp;\n",
    "    return intermediate_input\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n"
     ]
    }
   ],
   "source": [
    "for index in np.arange(0,3,1):\n",
    "    print(index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import RandomizedSearchCV\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Important hyperparam of random forest in decreasing order: <br>\n",
    "1.n_estimators <br>\n",
    "2.max_features <br>\n",
    "3.max_depth\n",
    "4.min_samples_split \n",
    "5.min_samples_leaf \n",
    "-- BootStrap #need to learn..ne\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def GridSearchRandomForest(X_train,y_train,trees=50):\n",
    "    base_random_forest=RandomForestClassifier(n_estimators=trees,random_state=108 )\n",
    "    #max_depth_val=[10,30,50,70,100]\n",
    "    max_depth_val=[10]\n",
    "    #min_samples_split_val=[2,4,8,12,16]\n",
    "    min_samples_split_val=[4]\n",
    "    #min_samples_leaf_val=[2,4,6,8,10]\n",
    "    min_samples_leaf_val=[2]\n",
    "    first_tuning={\"max_depth\":max_depth_val,\"min_samples_split\":min_samples_split_val,\"min_samples_leaf\":min_samples_leaf_val}\n",
    "    model1=GridSearchCV(estimator=base_random_forest,param_grid=first_tuning,cv=10)\n",
    "    model1.fit(X_train,y_train)\n",
    "    #n_estimators_val=[100,200,400,600]\n",
    "    n_estimators_val=[100]\n",
    "    max_features_val=[.5,1.0]\n",
    "    second_tuning={\"n_estimators\":n_estimators_val,\"max_features\":max_features_val}\n",
    "    final_model=GridSearchCV(estimator=model1.best_estimator_,param_grid=second_tuning,cv=10)\n",
    "    final_model.fit(X_train,y_train)\n",
    "    return final_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def RandomSearchRandomForest(X_train,y_train,trees=50):\n",
    "    n_estimators_val = [int(x) for x in np.linspace(start = 200, stop = 2000, num = 10)]\n",
    "    max_features_val = ['auto', 'sqrt'] #[.2,.4,.6,.8]\n",
    "    max_depth_val = [int(x) for x in np.linspace(10, 110, num = 11)]\n",
    "    max_depth_val.append(None)\n",
    "    min_samples_split_val = [2, 5, 10]\n",
    "    min_samples_leaf_val = [1, 2, 4]\n",
    "    bootstrap_val = [True, False]\n",
    "    random_grid={'n_estimators':n_estimators_val,'max_features':max_features_val,'max_depth':max_depth_val,\n",
    "                 'min_samples_split':min_samples_split_val,\n",
    "                 'min_samples_leaf':min_samples_leaf_val,\n",
    "                 'bootstrap':bootstrap_val\n",
    "                }\n",
    "    base_model=RandomForestClassifier(random_state=108)\n",
    "    random_model=RandomizedSearchCV(estimator=base_model,param_distributions=random_grid,n_iter=100,cv=10,random_state=108)\n",
    "    random_model.fit(X_train,y_train)\n",
    "    return random_model"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
