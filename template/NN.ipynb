{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "model.summary() Add dropout"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.  define problem - \n",
    "2.  assemble the data ->\n",
    "3.  choose the measure of success (Your metric for success will guide the choice of a loss function: what your model\n",
    "will optimize. It should directly align with your higher-level goals, such as the success\n",
    "of your business)\n",
    "\n",
    "4. decide evaluation protocol ( holdout set,k-fold cv or iterated k fold)\n",
    "5. Prepare the data (feature scaling,encoding,impuration,feature engg if possible)\n",
    "6. build a model better than base line  or random guessing (Last-layer activation,Loss function,Optimization configuration)\n",
    "7. Scaling up: developing a model that overfits (1 Add layers. 2 Make the layers bigger. 3 Train for more epochs)\n",
    "8. Regularizing your model and tuning your hyperparameters \n",
    "        Add dropout\n",
    "        Try different architectures: add or remove layers\n",
    "        Add L1 and/or L2 regularization\n",
    "        Try different hyperparameters (such as the number of units per layer or the learning rate of the optimizer) to find the optimal configuration\n",
    "        Optionally, iterate on feature engineering: add new features, or remove features that don’t seem to be informative."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For balanced-classification problems, where every class is equally likely, accuracy and\n",
    "area under the receiver operating characteristic curve (ROC AUC) are common metrics. For\n",
    "class-imbalanced problems, you can use precision and recall."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For instance, the widely used classification\n",
    "metric ROC AUC can’t be directly optimized. Hence, in classification tasks, it’s common\n",
    "to optimize for a proxy metric of ROC AUC, such as crossentropy. In general, you\n",
    "can hope that the lower the crossentropy gets, the higher the ROC AUC will be"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 0, 0],\n",
       "       [0, 1, 0, 1],\n",
       "       [1, 1, 0, 1],\n",
       "       [0, 0, 1, 0]])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.random.randint(0, high=2, size=(4,4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import models\n",
    "from keras import laye\n",
    "model = models.Sequential()\n",
    "model.add(layers.Dense(16, activation='relu', input_shape=(10000,)))\n",
    "model.add(layers.Dense(16, activation='relu'))\n",
    "model.add(layers.Dense(1, activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "l2(0.001) means every coefficient in the weight matrix of the layer will add 0.001 *\n",
    "weight_coefficient_value to the total loss of the network. Note that because this\n",
    "penalty is only added at training time, the loss for this network will be much higher at\n",
    "training than at test time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dropout rate is the fraction\n",
    "of the features that are zeroed out; it’s usually set between 0.2 and 0.5. At test time, no\n",
    "units are dropped out; instead, the layer’s output values are scaled down by a factor\n",
    "equal to the dropout rate, to balance for the fact that more units are active than at\n",
    "training time.\n",
    "\n",
    "layer_output *= np.random.randint(0, high=2, size=layer_output.shape)\n",
    "layer_output *= 0.5-----AT TEST TIME IF  THE VALUE IS NOT SCALED DOWN AT THE TRAINIA time\n",
    "so the more popular appraoch is \n",
    "layer_output /= 0.5 for input layer\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "most common ways to prevent overfitting in neural networks:\n",
    " Get more training data.\n",
    " Reduce the capacity of the network.\n",
    " Add weight regularization.\n",
    " Add dropout."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "This technique may seem strange and arbitrary. Why would this help reduce overfitting?\n",
    "Hinton says he was inspired by, among other things, a fraud-prevention mechanism\n",
    "used by banks. In his own words, “I went to my bank. The tellers kept changing\n",
    "and I asked one of them why. He said he didn’t know but they got moved around a lot."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import regularizers\n",
    "\n",
    "# regularizers.l1(0.001)\n",
    "# regularizers.l1_l2(l1=0.001, l2=0.001)\n",
    "seq_model=model.Sequential()\n",
    "seq_model.add(layers.Dense(16, kernel_regularizer=regularizers.l2(0.001),kernel_initializer='he_uniform',activation='relu', input_shape=(10000,)))\n",
    "model.add(layers.Dropout(0.5))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense,Dropout,Flatten\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "from keras import losses\n",
    "from keras import metrics\n",
    "from keras import optimizers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seq=model.Sequential()\n",
    "seq.add(16,kernel_initilizer='lecun_uniform',activation='elu',input_shape=(),name='layer1')\n",
    "seq.add(8,kernel_initilizer='lecun_uniform',activation='elu',name='layer2')\n",
    "seq.add(4,kernel_initilizer='lecun_uniform',activation='elu',name='layer3')\n",
    "seq.add(1,kernel_initilizer='lecun_uniform',activation='sigmoid',name='layer4')\n",
    "seq.compile(optimizer=optimizers.RMSprop(lr=0.001),loss=losses.binary_crossentropy,metrics=[metrics.binary_accuracy])\n",
    "history = model.fit(X_train, Y_train, validation_data=(X_test, Y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# best  selu and lecun\n",
    "df deepNeuralNetwork(df):\n",
    "    activation_hidden='elu'\n",
    "    activation_final='sigmoid'\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import losses\n",
    "from keras import metrics\n",
    "from keras import optimizers\n",
    "model.compile(optimizer=optimizers.RMSprop(lr=0.001),\n",
    "loss=losses.binary_crossentropy,\n",
    "metrics=[metrics.binary_accuracy])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kernel_initializer=['he_uniform','glorot','lecun_normal']\n",
    "he_avg_init = keras.initializers.VarianceScaling(scale=2., mode='fan_avg',\n",
    "distribution='uniform')\n",
    "keras.layers.Dense(10, activation=\"sigmoid\", kernel_initializer=he_avg_init)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "intermediate layer activations:\n",
    "    relu,leaky relu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Glorot               None, Tanh, Logistic, Softmax               1 / fanavg\n",
    "He                   ReLU & variants                            2 / fanin\n",
    "LeCun                 SELU                                      1 / fanin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SELU > ELU > leaky ReLU (and its variants) > ReLU > tanh\n",
    "> logistic. If the network’s architecture prevents it from selfnormalizing,\n",
    "then ELU may perform better than SELU (since SELU\n",
    "is not smooth at z = 0). If you care a lot about runtime latency, then\n",
    "you may prefer leaky ReLU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.models.Sequential([\n",
    "keras.layers.Flatten(input_shape=[28, 28]),\n",
    "keras.layers.BatchNormalization(),\n",
    "keras.layers.Dense(300, activation=\"elu\", kernel_initializer=\"he_normal\"),\n",
    "    \n",
    "    \n",
    "    \n",
    "model = keras.models.Sequential([\n",
    "keras.layers.Flatten(input_shape=[28, 28]),\n",
    "keras.layers.BatchNormalization(),\n",
    "keras.layers.Dense(300, kernel_initializer=\"he_normal\", use_bias=False),\n",
    "keras.layers.BatchNormalization(),\n",
    "keras.layers.Activation(\"elu\"),\n",
    "keras.layers.Dense(100, kernel_initializer=\"he_normal\", use_bias=False),"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "def plot_NN_matrix(history,value='loss')\n",
    "    if value == 'loss':\n",
    "        loss = history.history['loss']\n",
    "        val_loss = history.history['val_loss']\n",
    "        epochs = range(1, len(loss) + 1)\n",
    "        plt.plot(epochs, loss, 'bo', label='Training loss')\n",
    "        plt.plot(epochs, val_loss, 'b', label='Validation loss')\n",
    "        plt.title('Training and validation loss')\n",
    "        plt.xlabel('Epochs')\n",
    "        plt.ylabel('Loss')\n",
    "        plt.legend()\n",
    "        plt.show()\n",
    "        #plt.clf()\n",
    "    if value == 'acc':\n",
    "        acc = history.history['acc']\n",
    "        val_acc = history.history['val_acc']\n",
    "        epochs = range(1, len(acc) + 1)\n",
    "        plt.plot(epochs, acc, 'bo', label='Training acc')\n",
    "        plt.plot(epochs, val_acc, 'b', label='Validation acc')\n",
    "        plt.title('Training and validation accuracy')\n",
    "        plt.xlabel('Epochs')\n",
    "        plt.ylabel('Loss')\n",
    "        plt.legend()\n",
    "        plt.show()\n",
    "        \n",
    "        \n",
    "Listing 3.18 Training the model\n",
    "Listing 3.19 Plotting the training and validation loss\n",
    "Listing 3.20"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "problem type                                  last layer activation             Loss Function\n",
    "\n",
    "Binary classification   ----                 sigmoid                       ---- binary_crossentropy\n",
    "Multiclass, single-label classification----- softmax                       ----categorical_crossentropy\n",
    "Multiclass, multilabel classification ----   sigmoid                       ---- binary_crossentropy\n",
    "Regression to arbitrary values   ----         None                         ---- mse\n",
    "Regression to values between 0 and 1 ----    sigmoid                        ---- mse or binary_crossentropy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A Sequential model is not appropriate when:\n",
    "\n",
    "Your model has multiple inputs or multiple outputs\n",
    "Any of your layers has multiple inputs or multiple outputs\n",
    "You need to do layer sharing\n",
    "You want non-linear topology (e.g. a residual connection, a multi-branch model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'partial_x_train' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-6-d23b8b02f017>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mhistory\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpartial_x_train\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mpartial_y_train\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m20\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m512\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mvalidation_data\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_val\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_val\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'partial_x_train' is not defined"
     ]
    }
   ],
   "source": [
    "#history = model.fit(partial_x_train,partial_y_train,epochs=20,batch_size=512,validation_data=(x_val, y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#history_dict = history.history"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should also vectorize your labels, which is straightforward:\n",
    "y_train = np.asarray(train_labels).astype('float32')\n",
    "y_test = np.asarray(test_labels).astype('float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(X, Y, validation_split=0.33, epochs=150, batch_size=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "unexpected EOF while parsing (<ipython-input-1-e62fa2ee7390>, line 6)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-1-e62fa2ee7390>\"\u001b[1;36m, line \u001b[1;32m6\u001b[0m\n\u001b[1;33m    \u001b[0m\n\u001b[1;37m    ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m unexpected EOF while parsing\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def cross_validation(data,target,cv,model,epoch):\n",
    "    size=len(data)//cv\n",
    "    np.random.suffle(data)\n",
    "    validation_score=[]\n",
    "    for fold in range(cv):\n",
    "        validation_set=data[size*fold:size*(fold+1)]\n",
    "        validation_target_set=target[size*fold:size*(fold+1)]\n",
    "        training_set=np.concatenate([data[:size*fold],data[size*(fold+1):]],axis=0)\n",
    "        training_target_set=np.concatenate([target[:size*fold],target[size*(fold+1):]],axis=0)\n",
    "        model.fit(training_set,training_target_set,epochs=epoch,batch_size=1,verbose=0)\n",
    "        #validation_score=model.evaluate()\n",
    "        print(history.history.keys())\n",
    "        mae_history = history.history['val_mean_absolute_error']\n",
    "        all_mae_histories.append(mae_history)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "average_mae_history = [\n",
    "np.mean([x[i] for x in all_mae_histories]) for i in range(num_epochs)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
